{"cells":[{"cell_type":"markdown","metadata":{"id":"kchifhCic5bc"},"source":["# 1. What is PyTorch?"]},{"cell_type":"markdown","metadata":{"id":"PNChsFFhdbcx"},"source":["PyTorch is an open-source machine learning library developed by Facebook's AI Research lab.\n","\n","It provides a flexible and dynamic computational graph, which makes it particularly suited for research and prototyping.\n","\n","PyTorch is known for its simplicity and ease of use, making it a popular choice for both beginners and experienced researchers in the field of deep learning."]},{"cell_type":"markdown","metadata":{"id":"LgYpL7hUdhqc"},"source":["**Key Features**:\n","\n","  * **Dynamic Computational Graph**: A **dynamic computation graph is constructed in real-time during runtime**, while a static graph is pre-defined before execution. The adaptability of dynamic graphs facilitates complex architectures and simplifies debugging, whereas static graphs can optimize performance but may lack flexibility for certain model structures.\n","  * **Tensors**: PyTorch operates on multi-dimensional arrays called tensors, which are similar to NumPy's ndarrays but **with the ability to run on GPUs**.\n","  * **Neural Network Module**: PyTorch **provides torch.nn module** to define and train neural networks.\n","  * **GPU Acceleration**: PyTorch **supports GPU acceleration**, allowing computations to be performed much faster than on a CPU."]},{"cell_type":"markdown","metadata":{"id":"cAmEP2T4c8q9"},"source":["# 2. Why use PyTorch?"]},{"cell_type":"markdown","metadata":{"id":"qpKJXiJjdsiB"},"source":["While NumPy is an essential tool for numerical operations in Python, PyTorch offers additional features tailored for deep learning and neural network research:"]},{"cell_type":"markdown","metadata":{"id":"Qi4_inxiduSh"},"source":["**Autograd: Automatic Differentiation**\n","\n","* The most significant advantage of PyTorch over NumPy is its autograd system. **Autograd automatically computes the gradients or derivatives of operations**, which is essential for training neural networks using gradient-based optimization algorithms.\n","\n","* In deep learning, we often need to calculate the gradient of a loss function with respect to model parameters. **Manual computation can be error-prone and tedious**. Autograd simplifies this by automatically calculating gradients."]},{"cell_type":"markdown","metadata":{"id":"fEpS12tVeD5n"},"source":["##  Example:"]},{"cell_type":"markdown","metadata":{"id":"yXTf6qJNpvVZ"},"source":["Let's say we have a simple operation like $y = x^2$ as an example."]},{"cell_type":"markdown","metadata":{"id":"fGxKmr0lePs1"},"source":["NumPy (without Autograd)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1694485660641,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"Xq62wIUUeUWv","outputId":"d7f81e47-fdba-49a1-9d09-d423a909ee00"},"outputs":[{"name":"stdout","output_type":"stream","text":["[4.]\n"]}],"source":["import numpy as np\n","\n","# Define a value\n","#임의의 배열(array)를 설정\n","x_np = np.array([2.0])\n","\n","# Manually compute the gradient\n","dy_dx = 2 * x_np\n","\n","print(dy_dx)"]},{"cell_type":"markdown","metadata":{"id":"LCKZY05zeFq5"},"source":["PyTorch (with Autograd)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["## y.backward()의 의미는 다음으로 생각할 수 있다.\n","## Forward pass(정방향)의 계산을 진행할 때, x_pt -> y 순으로 계산이 이루어 짐을 알 수 있다.\n","## Backward pass(역방향)의 계산, 즉 gradient 계산을 진행한다면, 계산의 순서는 y -> x_pt가 되어 y.backward()의 계산이 x_pt.grad가 되는 것이다.\n","\n","## 쉽게 말해 backward()는 역방향 계산(gradient)을 의미하고\n","## .grad는 이로 계산된 최초 tensor의 gradient를 계산, 축적하여 반환한다.\n","\n","## 여기서 requires_grad=True는 grad가 축적되게 하라는 의미인데, 다른 말로 backward가 계속 주시하게 하라는 말이며, 지속해서 업데이트 된다는 말이다.\n","## 반면 이 옵션이 False라면 grad가 축적, 즉 업데이트 되지 않는다는 의미이고, 그 말은 다시 변하지 않는 상수라고 할 수 있겠다.\n","\n","## 정방향의 전파만을 수행하거나 텐서가 상수라면 requires_grad를 해제하거나 False로 설정하는 것이 바람직하다."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5782,"status":"ok","timestamp":1694485667712,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"zc0v9NYmeIar","outputId":"bf361cf0-83ec-4117-e9aa-4f747befc867"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([4.])\n"]}],"source":["import torch\n","\n","# Define a tensor and set requires_grad=True to track computation with it\n","x_pt = torch.tensor([2.0], requires_grad=True)\n","\n","# Define an operation\n","y = x_pt ** 2\n","\n","# Compute gradients\n","y.backward()\n","\n","# Display gradient\n","print(x_pt.grad)"]},{"cell_type":"markdown","metadata":{"id":"qx-19noieY9z"},"source":["While this is a simple example, imagine having to compute gradients for complex operations and architectures manually."]},{"cell_type":"markdown","metadata":{"id":"2gGxfaY1Nq3Q"},"source":["**Disabling Gradient Tracking**\n","\n","By default, all tensors with **requires_grad=True** are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with **torch.no_grad()** block:"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":306,"status":"ok","timestamp":1694485680254,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"XlWTPL3SNgev"},"outputs":[],"source":["x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)  # weight\n","b = torch.randn(3, requires_grad=True)  # bias"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1694485706917,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"ZmC6EJRIN5wF","outputId":"6d0804c6-7c34-4f4d-90c6-5225e7685333"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","False\n"]}],"source":["z = torch.matmul(x, w)+b    # 다차원 텐서 곱, 텐서의 행렬 계산\n","print(z.requires_grad)  # requires_grad 여부 확인\n","\n","with torch.no_grad():   # default인 requires_grad를 해제. with는 -일 때 정도로 해석\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"n0OqSlqAOC35"},"source":["Another way to achieve the same result is to use the **detach()** method on the tensor:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1694485737077,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"G5I3cmNdOGGJ","outputId":"bf71f7a5-09e0-48fe-89aa-e05e1cd87a84"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["z = torch.matmul(x, w)+b\n","z_det = z.detach()  # no_grad()와 동일\n","print(z_det.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"EkBYUU7WOPeH"},"source":["There are reasons you might want to disable gradient tracking:\n","- To mark some parameters in your neural network as frozen parameters.\n","- To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."]},{"cell_type":"markdown","metadata":{"id":"cjNGlug3eccd"},"source":["# 3. Handling **torch.Tensor**"]},{"cell_type":"markdown","metadata":{"id":"0raEateNfZgE"},"source":["In PyTorch, the fundamental object used for almost all computational tasks is the Tensor. A **Tensor** is similar to NumPy's **ndarray** but with the **added capability to be used on a GPU for faster computations**. Tensors are multi-dimensional arrays and are at the core of PyTorch's design."]},{"cell_type":"markdown","metadata":{"id":"gQZihPSbfdDa"},"source":["## a. Creating Tensors"]},{"cell_type":"markdown","metadata":{"id":"R6PRZzxEfgFL"},"source":["From Lists"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381,"status":"ok","timestamp":1694485863812,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"Sa3sURxZgVwG","outputId":"1d96c501-43ef-4582-d053-a27830a62366"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}],"source":["data = [[1, 2], [3, 4]]\n","tensor_from_data = torch.tensor(data)\n","\n","print(tensor_from_data)"]},{"cell_type":"markdown","metadata":{"id":"KimLkmhufh_T"},"source":["From NumPy Arrays"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1694485865471,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"RYippqM0gZbQ","outputId":"01b6c544-f277-49e3-b8a5-7e18574d9707"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 2]\n"," [3 4]]\n","tensor([[1, 2],\n","        [3, 4]], dtype=torch.int32)\n"]}],"source":["np_array = np.array(data)\n","print(np_array)\n","\n","tensor_from_numpy = torch.from_numpy(np_array)\n","print(tensor_from_numpy)"]},{"cell_type":"markdown","metadata":{"id":"IcVPJhhMfm7t"},"source":["Using Built-in Functions"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694485866733,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"99iJgn8EgfTk","outputId":"2de912c7-6d3e-4404-dab4-b3860253d4f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]])\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n","tensor([[0.0611, 0.9114, 0.0145],\n","        [0.9530, 0.2804, 0.8591],\n","        [0.0473, 0.6393, 0.8411]])\n"]}],"source":["tensor_zeros = torch.zeros(3, 3)\n","tensor_ones = torch.ones(3, 3)\n","tensor_eye = torch.eye(3)  # Identity matrix\n","tensor_rand = torch.rand(3, 3)  # Uniform random numbers between 0 and 1\n","\n","print(tensor_zeros)\n","print(tensor_ones)\n","print(tensor_eye)\n","print(tensor_rand)"]},{"cell_type":"markdown","metadata":{"id":"Qsbt9pmFfqWN"},"source":["## b. Tensor Operations"]},{"cell_type":"markdown","metadata":{"id":"TgGHLcQTfvg7"},"source":["**Arithmetic Operations**\n","\n","Arithmetic operations in PyTorch are element-wise operations similar to those in NumPy."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1694485870207,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"-3SRYAJcgkz6","outputId":"e97b22b0-1661-4091-b5a9-6481e024f847"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5, 7, 9])\n","tensor([-3, -3, -3])\n","tensor([ 4, 10, 18])\n","tensor([0.2500, 0.4000, 0.5000])\n"]}],"source":["x = torch.tensor([1, 2, 3])\n","y = torch.tensor([4, 5, 6])\n","\n","# Element-wise addition\n","z1 = x + y  # or torch.add(x, y)\n","print(z1)\n","# Output\n","# tensor([5, 7, 9])\n","\n","# Element-wise subtraction\n","z2 = x - y  # or torch.sub(x, y)\n","print(z2)\n","# Output\n","# tensor([-3, -3, -3])\n","\n","# Element-wise multiplication\n","z3 = x * y  # or torch.mul(x, y)\n","print(z3)\n","# Output\n","# tensor([4, 10, 18])\n","\n","# Element-wise division\n","z4 = x / y  # or torch.div(x, y)\n","print(z4)\n","# Output\n","# tensor([0.2500, 0.4000, 0.5000])"]},{"cell_type":"markdown","metadata":{"id":"JiYnjw-vfyHm"},"source":["**Reduction Operations**\n","\n","Reduction operations reduce the number of elements in a tensor."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1694485872659,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"OIInRQtVgmNg","outputId":"fed0755d-7d38-4bcb-a287-562301b7e234"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(10)\n","tensor(2.5000)\n","4 1\n"]}],"source":["x = torch.tensor([1, 2, 3, 4])\n","\n","# Sum of all elements\n","sum_all = torch.sum(x)\n","print(sum_all)\n","# Output\n","# tensor(10)\n","\n","# Mean of all elements (for float tensor)\n","mean_all = torch.mean(x.float())\n","print(mean_all)\n","# Output\n","# tensor(2.5000)\n","\n","# Max and Min values\n","max_val = torch.max(x).item() # .item = tensor에서 값만 가져오기\n","min_val = torch.min(x).item()\n","print(max_val, min_val)\n","# Output\n","# 4 1"]},{"cell_type":"markdown","metadata":{"id":"uXTS1-2uf0vk"},"source":["**Matrix Operations**\n","\n","Matrix operations are fundamental in deep learning, especially in neural network layers.\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694485874167,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"QN00yZgognfR","outputId":"bee0f085-b7ac-4f93-f1a4-5c8234fc147f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 4,  5],\n","        [10, 11]])\n","tensor([[2, 2],\n","        [3, 8]])\n","tensor([[1, 3],\n","        [2, 4]])\n"]}],"source":["mat1 = torch.tensor([[1, 2], [3, 4]])\n","mat2 = torch.tensor([[2, 1], [1, 2]])\n","\n","# Matrix multiplication\n","matmul_result = torch.mm(mat1, mat2)    # .matmul()과의 차이점: broadcast가 안됨(matrix의 사이즈가 같아야 함)\n","print(matmul_result)\n","\n","# Element-wise matrix multiplication\n","elementwise_mul = mat1 * mat2\n","print(elementwise_mul)\n","\n","# Matrix transpose\n","transpose_mat = torch.t(mat1)\n","print(transpose_mat)"]},{"cell_type":"markdown","metadata":{"id":"hVuZL9Bbf2bp"},"source":["**Reshaping**\n","\n","Reshaping allows you to change the shape (number of dimensions and size along each dimension) of a tensor.\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1694485876144,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"Z6OoByS2go_4","outputId":"d22d498c-6e9c-44fd-aefa-c9fc6e5d5053"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1],\n","        [2],\n","        [3],\n","        [4]])\n","torch.Size([4, 1])\n","tensor([1, 2, 3, 4])\n","torch.Size([4])\n"]}],"source":["x = torch.tensor([[1, 2], [3, 4]])\n","\n","# Reshape to 4x1 tensor\n","# This stacks the rows of 'x' into a single column.\n","reshaped = x.view(4, 1)     # contiguous(axis 방향으로 자료가 저장된 상태) 성질을 만족해야 사용 가능\n","print(reshaped)\n","print(reshaped.shape)\n","\n","# Flatten the tensor\n","# This transforms 'x' into a 1D tensor by unrolling its values.\n","# The '-1' in view indicates to infer the size for that dimension based on the original tensor.\n","flattened = x.view(-1)\n","print(flattened)\n","print(flattened.shape)"]},{"cell_type":"markdown","metadata":{"id":"OYN49SE0jC91"},"source":["The reshape function is similar to **view** but provides more flexibility as it returns a new tensor with the desired shape. If the requested shape is compatible with the original tensor and no memory copy is needed, it will share the same data; otherwise, a copy will be made."]},{"cell_type":"markdown","metadata":{"id":"MuTJFkT2tkTt"},"source":["**Squeeze** and **Unsqueeze**\n","\n","**squeeze** function removes dimensions of size 1 from a tensor's shape. It's useful to reduce unnecessary dimensions.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456,"status":"ok","timestamp":1694485878817,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"-1S_2UBlt7XO","outputId":"74091b56-46ca-4ace-8c7c-77b9c794d5b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 1])\n","torch.Size([3])\n","tensor([1, 2, 3])\n"]}],"source":["# .squeeze() = 차원이 1인 경우 해당 차원을 제거\n","\n","x = torch.tensor([[1], [2], [3]])\n","print(x.shape)\n","\n","squeezed_tensor = x.squeeze()\n","print(squeezed_tensor.shape)\n","\n","print(squeezed_tensor)\n","\n","# 2차원 벡터에서 1차원 벡터로 변경됨을 알 수 있다."]},{"cell_type":"markdown","metadata":{"id":"swu-ULRiuFMp"},"source":["\n"," As the opposite of squeeze, **unsqueeze** function adds a dimension of size 1 to a tensor's shape at a specified position."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1694485880214,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"x817pCg2uGs_","outputId":"0ef9fbc6-2427-4490-aa03-6a3b7c35d177"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3])\n","\n","torch.Size([1, 3])\n","tensor([[1, 2, 3]])\n","\n","torch.Size([3, 1])\n","tensor([[1],\n","        [2],\n","        [3]])\n"]}],"source":["# 하지만 필요시에 나타나게 만들 수 있다.\n","\n","x = torch.tensor([1, 2, 3])\n","print(x.shape)\n","\n","print()\n","\n","unsqueezed_tensor1 = x.unsqueeze(dim=0)     # 괄호 안에는 생성할 차원(Dim)을 정해주어야 한다.\n","print(unsqueezed_tensor1.shape)\n","\n","print(unsqueezed_tensor1)\n","\n","print()\n","\n","unsqueezed_tensor2 = x.unsqueeze(dim=1)     # Dim=0: 행, Dim=1: 열\n","print(unsqueezed_tensor2.shape)\n","\n","print(unsqueezed_tensor2)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694485881509,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"YBEosZh0jH3Z","outputId":"aaae8a84-a839-4ce5-c4a6-e7a56637ebf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","torch.Size([2, 3])\n"]}],"source":["x_reshape = torch.tensor([1, 2, 3, 4, 5, 6])\n","reshaped_tensor = x_reshape.reshape(2, 3)\n","print(reshaped_tensor)\n","print(reshaped_tensor.shape)"]},{"cell_type":"markdown","metadata":{"id":"o3_D5oHrf3jh"},"source":["**Concatenation**\n","\n","You can concatenate multiple tensors along a specific dimension."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694485882962,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"nDSodNytgqi3","outputId":"99292c0f-3b22-4134-900d-b2d3b369e041"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3, 4, 5, 6])\n","torch.Size([6])\n","tensor([[1],\n","        [2],\n","        [3]])\n","tensor([[4],\n","        [5],\n","        [6]])\n","torch.Size([3, 1])\n","torch.Size([3, 1])\n","tensor([[1, 4],\n","        [2, 5],\n","        [3, 6]])\n","torch.Size([3, 2])\n"]}],"source":["x = torch.tensor([1, 2, 3])\n","y = torch.tensor([4, 5, 6])\n","\n","# Concatenate along dimension 0\n","concatenated_dim0 = torch.cat([x, y], dim=0)    # 0차원(행)에 대해서 결합\n","print(concatenated_dim0)\n","# Output\n","# tensor([1, 2, 3, 4, 5, 6])\n","print(concatenated_dim0.shape)\n","\n","# If you have 2D tensors, you can concatenate along dimension 1 as well\n","x2 = x.unsqueeze(1)\n","y2 = y.unsqueeze(1)\n","print(x2, y2, sep='\\n')\n","print(x2.shape, y2.shape, sep='\\n')\n","\n","concatenated_dim1 = torch.cat([x2, y2], dim=1)\n","print(concatenated_dim1)\n","print(concatenated_dim1.shape)"]},{"cell_type":"markdown","metadata":{"id":"u_lOKMgOf5_Q"},"source":["**Indexing, Slicing, Joining, and Mutating**\n","\n","These operations allow you to access and modify specific parts of a tensor.\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1694485885564,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"ZdEpWHcXgrsz","outputId":"434272eb-c0b7-4e2e-c88d-122859019461"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(20)\n","tensor([20, 30])\n","tensor([[10, 20, 30, 40, 50],\n","        [10, 20, 30, 40, 50]])\n","tensor([100,  20,  30,  40,  50])\n"]}],"source":["x = torch.tensor([10, 20, 30, 40, 50])\n","\n","# Indexing: Get the second element\n","second_element = x[1]\n","print(second_element)\n","# Output\n","# tensor(20)\n","\n","# Slicing: Get the second and third elements\n","slice_2_3 = x[1:3]\n","print(slice_2_3)\n","# Output\n","# tensor([20, 30])\n","\n","# Joining: Stack tensors together\n","stacked = torch.stack([x, x])\n","print(stacked)\n","# Output\n","# tensor([[10, 20, 30, 40, 50],\n","#         [10, 20, 30, 40, 50]])\n","\n","# Mutating: Change the first element of x to 100\n","x[0] = 100\n","print(x)\n","# Output\n","# tensor([100,  20,  30,  40,  50])"]},{"cell_type":"markdown","metadata":{"id":"AsphD0_OgBAw"},"source":["## c. Broadcasting"]},{"cell_type":"markdown","metadata":{"id":"OBd1YtzegDmX"},"source":["PyTorch supports broadcasting, a feature borrowed from NumPy. It allows PyTorch to work with arrays of different shapes when performing arithmetic operations."]},{"cell_type":"markdown","metadata":{"id":"xhBJ-HSNut53"},"source":["### Examples"]},{"cell_type":"markdown","metadata":{"id":"OQGXRkHEuyCL"},"source":["**Scalar and Tensor**\n","\n","Adding a scalar to a tensor broadcasts the scalar across all elements of the tensor."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":320,"status":"ok","timestamp":1694485889235,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"XceSd5Fru4D3","outputId":"64e63cdc-0ccb-4125-f3a8-71fd40c04ebc"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([3, 4, 5])\n"]}],"source":["tensor = torch.tensor([1, 2, 3])\n","result = tensor + 2\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"Q3qKkOyWu6tp"},"source":["**Tensors with Different Dimensions**\n","\n","Adding a tensor of shape [3, 1] to a tensor of shape [3] will broadcast the second tensor across the rows of the first one."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694485891006,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"f3VWvqSAvBDH","outputId":"e72ad1d2-3e3a-48e6-f419-9d53c768fe82"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 1]) torch.Size([3])\n","tensor([[5, 6, 7],\n","        [6, 7, 8],\n","        [7, 8, 9]])\n","torch.Size([3, 3])\n"]}],"source":["tensor_a = torch.tensor([[1], [2], [3]])\n","tensor_b = torch.tensor([4, 5, 6])\n","print(tensor_a.shape, tensor_b.shape)\n","\n","result = tensor_a + tensor_b\n","print(result)\n","print(result.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"EP7Ka8q9vSXc"},"source":["**Tensors with Mismatched Dimensions**\n","\n","Operations between tensors with mismatched dimensions where neither dimension is 1 will raise an error."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":440,"status":"ok","timestamp":1694485893142,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"AvCWXOdTvWPH","outputId":"0b14305b-5350-4113-f611-71c20710ee59"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 2]) torch.Size([3])\n","The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"]}],"source":["tensor_c = torch.tensor([[1, 2], [3, 4]])\n","tensor_d = torch.tensor([1, 2, 3])\n","print(tensor_c.shape, tensor_d.shape)\n","\n","# This will raise an error: result = tensor_c + tensor_d\n","try:\n","    result = tensor_c + tensor_d\n","except RuntimeError as e:\n","    error_message = str(e)\n","    print(error_message)"]},{"cell_type":"markdown","metadata":{"id":"QnX_xDZ7gFok"},"source":["## d. Moving Tensors between CPU and GPU"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694485896017,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"Hv6rV-SLwSQO","outputId":"4e29f84f-d217-4ff0-b395-6ee39e1280c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["import torch\n","print(torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"PX0rYNgfgKV4"},"source":["**Moving a Tensor to GPU**\n","\n","PyTorch uses CUDA (a parallel computing platform and application programming interface model created by NVIDIA) to enable GPU computations. To move a tensor to the GPU, the cuda() method is used."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6442,"status":"ok","timestamp":1694485905588,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"DhvUTYFRwRly","outputId":"f8e7d8e0-6bd6-4dd1-b1f4-04a8638889f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n","cuda:0\n"]}],"source":["# Create a tensor on CPU\n","tensor_cpu = torch.tensor([1, 2, 3])\n","print(tensor_cpu.device)\n","\n","# Move the tensor to GPU (if CUDA is available)\n","if torch.cuda.is_available():\n","    tensor_gpu = tensor_cpu.cuda()\n","    print(tensor_gpu.device)"]},{"cell_type":"markdown","metadata":{"id":"-B_3q0nEw43k"},"source":["**Moving a Tensor to a Specific GPU**\n","\n","In systems with multiple GPUs, they are numbered as 0, 1, 2, etc. You can select a specific GPU by setting its ID. Once the device is set to a specific GPU ID, you can move the tensor to that GPU using the to() method."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":532,"status":"ok","timestamp":1694485908665,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"sLBlZf3Pw-13","outputId":"7d249c33-4ee7-44ed-9903-739205039317"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n","cuda:0\n"]}],"source":["# Set the GPU ID (for this example, we'll set it to 0)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# Move the tensor to the specified GPU\n","tensor_gpu_specific = tensor_cpu.to(device)\n","print(tensor_gpu_specific.device)"]},{"cell_type":"markdown","metadata":{"id":"T0228-fyworo"},"source":["**Moving a Tensor back to CPU**\n","\n","Once the computations on the GPU are completed, you might want to move the results back to the CPU. This can be achieved using the cpu() method."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694485910712,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"4K8lmd-cwqyi","outputId":"01d10066-e3cd-4e2c-c259-828ce046893b"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# Move the tensor back to CPU from GPU\n","tensor_cpu_again = tensor_gpu.cpu()\n","print(tensor_cpu_again.device)"]},{"cell_type":"markdown","metadata":{"id":"Dnv3xjFJxdsU"},"source":["If the tensors are not on the same device, you cannot perform operations on them."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694485911985,"user":{"displayName":"­홍민기 | 인공지능학과 | 한양대(서울)","userId":"13840019901450441786"},"user_tz":-540},"id":"2ZUxbwdIxiHZ","outputId":"684b8d05-0baf-4bf0-d675-2aeae078bcfa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"]}],"source":["# Assuming you have a CUDA-enabled GPU\n","if torch.cuda.is_available():\n","    # Create a tensor on the CPU\n","    tensor_cpu = torch.tensor([1, 2, 3])\n","\n","    # Move the tensor to GPU\n","    tensor_gpu = tensor_cpu.cuda()\n","\n","    # Attempt to add the CPU tensor and GPU tensor\n","    # This will raise an error because the tensors are on different devices\n","    try:\n","        result = tensor_cpu + tensor_gpu\n","    except RuntimeError as e:\n","        error_message = str(e)\n","        print(error_message)"]},{"cell_type":"markdown","metadata":{"id":"YF71zcuroXmq"},"source":["# 4. Exercise"]},{"cell_type":"markdown","metadata":{"id":"NSHmfACoohZv"},"source":["1. Create two tensors of size (3, 3) with random values. Multiply them element-wise."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"-ra84yddokcz"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"_UGOu1mmoqfv"},"source":["2. Create a tensor **A** of size (5, 3) and another tensor **B** of size (3,). Add them."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"mLeL9KLxoxxU"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"K9vgaVvMoyJe"},"source":["3. Create two tensors of size (4, 3) and (3, 2). Perform matrix multiplication."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"WOv71jmmo2oX"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"ztjBFrP6o5O_"},"source":["4. Create a tensor of size (2, 6). Reshape it to (3, 4)."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"TgNcV1aio7SW"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"71zW1KKJo-XF"},"source":["5. Create a tensor **A** of size (5, 1) and another tensor **B** of size (5,). Perform element-wise multiplication."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"aUDIpBHUpC_B"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"NoGQuBgPpCq4"},"source":["6. Create two tensors of size (3, 4). Transpose the second tensor and perform matrix multiplication."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"i5yTYcp4pFh7"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"onGizY85pGig"},"source":["7. Create a tensor of size (3, 1) and expand its size to (3, 4). Sum the elements along the second dimension."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"PIfhLB6TpKgE"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"6Q9bSCM9pL7r"},"source":["8. Create a tensor of size (6, 5). Calculate the mean along the second dimension."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"VCh7H308pLtK"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"8y6RsNIYpOiF"},"source":["9. Create a tensor of size (4, 4). Extract the second and fourth row."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"XXuInXf7pQ9z"},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"7qFf8A-jpSIU"},"source":["10. Calculate the outer product of two tensors of size (5,) and (3,).\n","\n","- *Search for the outer product function on Google!*"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"-4_Jnd1HpUU8"},"outputs":[],"source":["# Write your code here\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
